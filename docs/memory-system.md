# è®°å¿†ç³»ç»Ÿè®¾è®¡

INVESTOR-BENCHçš„æ ¸å¿ƒåˆ›æ–°æ˜¯å››å±‚åˆ†çº§è®°å¿†ç³»ç»Ÿï¼Œæ¨¡æ‹Ÿäººç±»æŠ•èµ„ä¸“å®¶çš„è®¤çŸ¥è®°å¿†æ¨¡å¼ï¼Œå®ç°é•¿æœŸæŠ•èµ„çŸ¥è¯†çš„ç§¯ç´¯å’Œåº”ç”¨ã€‚

## ğŸ§  è®°å¿†ç³»ç»Ÿæ¦‚è§ˆ

### è®¾è®¡ç†å¿µ

åŸºäºè®¤çŸ¥ç§‘å­¦å’Œè¡Œä¸ºé‡‘èå­¦åŸç†ï¼Œæ¨¡æ‹Ÿä¸“ä¸šæŠ•èµ„è€…çš„è®°å¿†ç»“æ„ï¼š

1. **çŸ­æœŸè®°å¿†** - å³æ—¶ååº”å’Œæ—¥å¸¸ä¿¡æ¯
2. **ä¸­æœŸè®°å¿†** - è¶‹åŠ¿åˆ†æå’Œå‘¨æœŸæ€§è§„å¾‹  
3. **é•¿æœŸè®°å¿†** - åŸºæœ¬é¢çŸ¥è¯†å’Œå†å²ç»éªŒ
4. **åæ€è®°å¿†** - æŠ•èµ„å“²å­¦å’Œç­–ç•¥åŸåˆ™

```mermaid
graph TB
    A[æ–°ä¿¡æ¯è¾“å…¥] --> B{é‡è¦æ€§è¯„ä¼°}
    
    B -->|ä½é‡è¦æ€§<55| C[çŸ­æœŸè®°å¿†]
    B -->|ä¸­é‡è¦æ€§55-85| D[ä¸­æœŸè®°å¿†]
    B -->|é«˜é‡è¦æ€§>85| E[é•¿æœŸè®°å¿†]
    
    C --> F[è¡°å‡æœºåˆ¶]
    D --> F  
    E --> F
    
    F --> G[è®°å¿†æµåŠ¨]
    G --> H[è®°å¿†æ•´åˆ]
    H --> I[åæ€è®°å¿†]
    
    J[å†³ç­–è¯·æ±‚] --> K[è®°å¿†æ£€ç´¢]
    K --> C
    K --> D
    K --> E  
    K --> I
    
    K --> L[ç›¸å…³è®°å¿†]
    L --> M[LLMå†³ç­–]
```

## ğŸ“š å››å±‚è®°å¿†è¯¦è§£

### 1. çŸ­æœŸè®°å¿† (Short Memory)

**æ—¶é—´èŒƒå›´**: 1-7å¤©  
**å­˜å‚¨å®¹é‡**: æ— é™åˆ¶ï¼Œä½†å¿«é€Ÿè¡°å‡  
**ä¸»è¦å†…å®¹**: 
- æ¯æ—¥æ–°é—»å’Œå¸‚åœºåŠ¨æ€
- çŸ­æœŸä»·æ ¼æ³¢åŠ¨
- å³æ—¶æƒ…ç»ªååº”
- æ—¥å†…äº¤æ˜“ä¿¡å·

#### é…ç½®å‚æ•°
```json
{
  "db_name": "short",
  "importance_init_val": 50.0,        // åˆå§‹é‡è¦æ€§åˆ†æ•°
  "decay_recency_factor": 3.0,        // 3å¤©è¡°å‡å‘¨æœŸ
  "decay_importance_factor": 0.92,    // æ¯å¤©è¡°å‡8%
  "clean_up_recency_threshold": 0.05, // æ¸…ç†é˜ˆå€¼
  "clean_up_importance_threshold": 5.0,
  "jump_upper_threshold": 55.0        // æ™‹å‡åˆ°ä¸­æœŸè®°å¿†é˜ˆå€¼
}
```

#### è¡°å‡è®¡ç®—å…¬å¼
```python
# æ–°è¿‘æ€§è¡°å‡
recency_score = exp(-days_elapsed / decay_recency_factor)

# é‡è¦æ€§è¡°å‡  
importance_score = initial_importance * (decay_importance_factor ** days_elapsed)

# ç»¼åˆåˆ†æ•°
final_score = importance_score * recency_score
```

#### å…¸å‹å­˜å‚¨å†…å®¹ç¤ºä¾‹
```json
{
  "memory_id": 1,
  "content": "è‹¹æœå…¬å¸å‘å¸ƒæ–°iPhoneï¼Œè‚¡ä»·ç›˜å‰ä¸Šæ¶¨2.5%",
  "date": "2023-09-15",
  "importance": 52.0,
  "symbol": "AAPL",
  "sentiment": "positive",
  "vector": [0.123, -0.456, ...],
  "metadata": {
    "source": "news",
    "category": "product_launch"
  }
}
```

### 2. ä¸­æœŸè®°å¿† (Mid Memory)

**æ—¶é—´èŒƒå›´**: 1å‘¨-3ä¸ªæœˆ  
**å­˜å‚¨ç­–ç•¥**: ä»çŸ­æœŸè®°å¿†æ™‹å‡æˆ–ç›´æ¥å­˜å‚¨  
**ä¸»è¦å†…å®¹**:
- å­£åº¦è´¢æŠ¥åˆ†æ
- è¡Œä¸šè¶‹åŠ¿å˜åŒ–  
- å®è§‚ç»æµæ•°æ®
- ä¸­æœŸæŠ€æœ¯å½¢æ€

#### é…ç½®å‚æ•°
```json
{
  "db_name": "mid", 
  "importance_init_val": 60.0,
  "decay_recency_factor": 90.0,       // 90å¤©è¡°å‡å‘¨æœŸ
  "decay_importance_factor": 0.96,    // æ¯å¤©è¡°å‡4%
  "jump_lower_threshold": 55.0,       // ä»çŸ­æœŸè®°å¿†æ¥æ”¶é˜ˆå€¼
  "jump_upper_threshold": 85.0        // æ™‹å‡åˆ°é•¿æœŸè®°å¿†é˜ˆå€¼
}
```

#### è®°å¿†æµåŠ¨æœºåˆ¶
```python
def memory_flow_check(memory):
    if memory.importance_score > jump_upper_threshold:
        # æ™‹å‡åˆ°é•¿æœŸè®°å¿†
        move_to_long_memory(memory)
    elif memory.importance_score < jump_lower_threshold:
        # é™çº§åˆ°çŸ­æœŸè®°å¿†æˆ–æ¸…ç†
        if memory.recency_score > 0.1:
            move_to_short_memory(memory)
        else:
            delete_memory(memory)
```

### 3. é•¿æœŸè®°å¿† (Long Memory)

**æ—¶é—´èŒƒå›´**: 3ä¸ªæœˆä»¥ä¸Šï¼ŒæŒä¹…å­˜å‚¨  
**å­˜å‚¨ç­–ç•¥**: é«˜é‡è¦æ€§ä¿¡æ¯çš„é•¿æœŸä¿å­˜  
**ä¸»è¦å†…å®¹**:
- å…¬å¸åŸºæœ¬é¢åˆ†æ
- é•¿æœŸæŠ•èµ„ç†å¿µ
- å†å²é‡å¤§äº‹ä»¶
- ç»æµå‘¨æœŸè§„å¾‹

#### é…ç½®å‚æ•°
```json
{
  "db_name": "long",
  "importance_init_val": 90.0,
  "decay_recency_factor": 365.0,      // 365å¤©è¡°å‡å‘¨æœŸ  
  "decay_importance_factor": 0.96,    // ææ…¢è¡°å‡
  "jump_lower_threshold": 85.0,       // æ¥æ”¶é˜ˆå€¼
  "clean_up_importance_threshold": 20.0 // æ›´é«˜çš„æ¸…ç†é˜ˆå€¼
}
```

#### é•¿æœŸè®°å¿†ç‰¹ç‚¹
- **æŒä¹…æ€§**: è¡°å‡ææ…¢ï¼Œå¯ä¿å­˜æ•°å¹´
- **ç¨³å®šæ€§**: ä¸æ˜“è¢«çŸ­æœŸæ³¢åŠ¨å½±å“
- **æƒå¨æ€§**: åœ¨å†³ç­–ä¸­å…·æœ‰æ›´é«˜æƒé‡
- **æ¦‚æ‹¬æ€§**: å­˜å‚¨æŠ½è±¡çš„æŠ•èµ„åŸåˆ™å’Œè§„å¾‹

### 4. åæ€è®°å¿† (Reflection Memory)

**ç‹¬ç‰¹åŠŸèƒ½**: å­˜å‚¨å…ƒè®¤çŸ¥å’ŒæŠ•èµ„å“²å­¦  
**è§¦å‘æœºåˆ¶**: å®šæœŸåæ€æˆ–é‡å¤§å†³ç­–å  
**ä¸»è¦å†…å®¹**:
- æŠ•èµ„å†³ç­–æ€»ç»“
- æˆåŠŸ/å¤±è´¥ç»éªŒ
- ç­–ç•¥è°ƒæ•´æ€è€ƒ
- æŠ•èµ„å“²å­¦è¿›åŒ–

#### é…ç½®å‚æ•°
```json
{
  "db_name": "reflection",
  "importance_init_val": 80.0,
  "decay_recency_factor": 365.0,
  "decay_importance_factor": 0.98,    // æœ€æ…¢è¡°å‡
  "similarity_threshold": 0.95        // é«˜ç›¸ä¼¼åº¦å»é‡
}
```

#### åæ€è§¦å‘æ¡ä»¶
```python
def should_trigger_reflection(agent_state):
    conditions = [
        agent_state.days_since_last_reflection > 30,  # 30å¤©æœªåæ€
        agent_state.recent_loss > 0.05,               # æœ€è¿‘æŸå¤±>5%
        agent_state.strategy_change_detected,         # æ£€æµ‹åˆ°ç­–ç•¥å˜åŒ–
        agent_state.market_regime_change              # å¸‚åœºç¯å¢ƒå˜åŒ–
    ]
    return any(conditions)
```

## ğŸ”„ è®°å¿†æ“ä½œæœºåˆ¶

### 1. è®°å¿†å­˜å‚¨æµç¨‹

```mermaid
sequenceDiagram
    participant Agent as FinMemAgent
    participant Embed as EmbeddingService  
    participant Memory as MemoryManager
    participant Qdrant as VectorDB
    
    Agent->>Embed: æ–‡æœ¬å‘é‡åŒ–
    Embed-->>Agent: è¿”å›å‘é‡
    Agent->>Memory: å­˜å‚¨è¯·æ±‚
    Memory->>Memory: é‡è¦æ€§è¯„åˆ†
    Memory->>Memory: ç¡®å®šå­˜å‚¨å±‚çº§
    Memory->>Qdrant: å†™å…¥å‘é‡æ•°æ®åº“
    Qdrant-->>Memory: ç¡®è®¤å­˜å‚¨
    Memory-->>Agent: å­˜å‚¨å®Œæˆ
```

#### é‡è¦æ€§è¯„åˆ†ç®—æ³•
```python
def calculate_importance(content, context):
    factors = {
        'market_impact': assess_market_impact(content),      # 0-30åˆ†
        'information_novelty': assess_novelty(content),      # 0-25åˆ†  
        'relevance_to_holdings': assess_relevance(content),  # 0-20åˆ†
        'source_credibility': assess_source(content),        # 0-15åˆ†
        'temporal_urgency': assess_urgency(content)          # 0-10åˆ†
    }
    
    weighted_score = sum(score * weight for score, weight in factors.items())
    return min(weighted_score, 100.0)  # æœ€é«˜100åˆ†
```

### 2. è®°å¿†æ£€ç´¢æµç¨‹

```python
def retrieve_relevant_memories(query, top_k=5):
    # 1. å‘é‡åŒ–æŸ¥è¯¢
    query_vector = embedding_service.embed(query)
    
    # 2. å¤šå±‚çº§æ£€ç´¢
    memories = {}
    for memory_type in ['short', 'mid', 'long', 'reflection']:
        results = qdrant_client.search(
            collection_name=f"{symbol}_{memory_type}",
            query_vector=query_vector,
            limit=top_k,
            score_threshold=0.7
        )
        memories[memory_type] = results
    
    # 3. é‡è¦æ€§åŠ æƒæ’åº
    all_memories = []
    for mem_type, mems in memories.items():
        for mem in mems:
            weighted_score = mem.score * memory_weights[mem_type]
            all_memories.append((weighted_score, mem))
    
    # 4. è¿”å›æœ€ç›¸å…³çš„è®°å¿†
    return sorted(all_memories, reverse=True)[:top_k]
```

### 3. è®°å¿†ç»´æŠ¤æœºåˆ¶

#### å®šæœŸæ¸…ç†ä»»åŠ¡
```python
def memory_maintenance():
    for collection in memory_collections:
        # 1. æ¸…ç†è¿‡æœŸè®°å¿†
        expired_memories = find_expired_memories(collection)
        delete_memories(expired_memories)
        
        # 2. è®°å¿†æµåŠ¨æ£€æŸ¥
        promote_memories = find_promotion_candidates(collection)
        for memory in promote_memories:
            move_to_higher_level(memory)
        
        # 3. å»é‡å¤„ç†
        duplicates = find_duplicate_memories(collection)
        merge_or_delete_duplicates(duplicates)
        
        # 4. é‡æ–°è®¡ç®—é‡è¦æ€§åˆ†æ•°
        update_importance_scores(collection)
```

#### è®°å¿†å‹ç¼©å’Œå½’æ¡£
```python  
def memory_compression():
    # å°†ç›¸ä¼¼çš„çŸ­æœŸè®°å¿†åˆå¹¶ä¸ºä¸­æœŸè®°å¿†
    similar_groups = cluster_similar_memories('short')
    for group in similar_groups:
        if len(group) > 3:  # 3ä¸ªä»¥ä¸Šç›¸ä¼¼è®°å¿†
            compressed_memory = create_summary_memory(group)
            store_in_mid_memory(compressed_memory)
            delete_original_memories(group)
```

## ğŸ“Š è®°å¿†ç³»ç»Ÿæ€§èƒ½

### å­˜å‚¨å®¹é‡è§„åˆ’

| è®°å¿†å±‚çº§ | å¹³å‡è®°å¿†æ•° | ç”Ÿå‘½å‘¨æœŸ | å­˜å‚¨å¤§å° |
|---------|-----------|----------|----------|
| çŸ­æœŸè®°å¿† | 100-500   | 1-7å¤©    | 10-50MB  |
| ä¸­æœŸè®°å¿† | 200-800   | 1å‘¨-3æœˆ  | 20-80MB  |
| é•¿æœŸè®°å¿† | 500-2000  | 3æœˆ-æ•°å¹´  | 50-200MB |
| åæ€è®°å¿† | 50-200    | æŒä¹…     | 5-20MB   |

### æ£€ç´¢æ€§èƒ½ä¼˜åŒ–

#### å‘é‡ç´¢å¼•ä¼˜åŒ–
```python
# Qdranté›†åˆé…ç½®
collection_config = {
    "vectors": {
        "size": 2560,  # å‘é‡ç»´åº¦
        "distance": "Cosine"  # ä½™å¼¦ç›¸ä¼¼åº¦
    },
    "optimizers_config": {
        "default_segment_number": 2,
        "max_segment_size": 20000,
        "memmap_threshold": 50000
    },
    "hnsw_config": {
        "m": 16,         # è¿æ¥æ•°
        "ef_construct": 100,  # æ„å»ºæ—¶æœç´¢å‚æ•°
        "full_scan_threshold": 10000
    }
}
```

#### ç¼“å­˜ç­–ç•¥
```python
from functools import lru_cache

@lru_cache(maxsize=1000)
def cached_memory_search(query_hash, memory_type):
    """ç¼“å­˜å¸¸è§æŸ¥è¯¢ç»“æœ"""
    return perform_vector_search(query_hash, memory_type)
```

## ğŸ¯ è®°å¿†ç³»ç»Ÿå®æˆ˜åº”ç”¨

### å†³ç­–ä¸­çš„è®°å¿†è¿ç”¨

#### 1. å¤šå±‚è®°å¿†èåˆå†³ç­–
```python
def make_investment_decision(current_info):
    # æ£€ç´¢å„å±‚è®°å¿†
    short_memories = retrieve_memories(current_info, 'short', top_k=3)
    mid_memories = retrieve_memories(current_info, 'mid', top_k=2) 
    long_memories = retrieve_memories(current_info, 'long', top_k=2)
    reflection_memories = retrieve_memories(current_info, 'reflection', top_k=1)
    
    # æ„å»ºå†³ç­–ä¸Šä¸‹æ–‡
    context = {
        'current': current_info,
        'recent_trends': short_memories,
        'industry_analysis': mid_memories, 
        'fundamental_knowledge': long_memories,
        'investment_philosophy': reflection_memories
    }
    
    # LLMå†³ç­–
    return llm_decision(context)
```

#### 2. è®°å¿†ä¸€è‡´æ€§æ£€æŸ¥
```python
def check_memory_consistency(new_decision, historical_memories):
    """æ£€æŸ¥æ–°å†³ç­–ä¸å†å²è®°å¿†çš„ä¸€è‡´æ€§"""
    
    consistency_scores = []
    for memory in historical_memories:
        similarity = calculate_semantic_similarity(new_decision, memory.content)
        sentiment_alignment = compare_sentiment(new_decision, memory)
        
        consistency = (similarity * 0.7 + sentiment_alignment * 0.3)
        consistency_scores.append(consistency)
    
    avg_consistency = sum(consistency_scores) / len(consistency_scores)
    
    if avg_consistency < 0.3:
        # å†³ç­–ä¸å†å²è®°å¿†ä¸ä¸€è‡´ï¼Œéœ€è¦ç‰¹åˆ«å…³æ³¨
        trigger_reflection("Decision inconsistency detected")
    
    return avg_consistency
```

### è®°å¿†ç³»ç»Ÿç›‘æ§

#### å…³é”®æŒ‡æ ‡ç›‘æ§
```python
def memory_system_health_check():
    metrics = {
        'total_memories': count_all_memories(),
        'memory_distribution': get_memory_distribution(),
        'average_importance': calculate_avg_importance(),
        'retrieval_latency': measure_retrieval_speed(),
        'storage_utilization': get_storage_usage(),
        'memory_flow_rate': calculate_promotion_rate()
    }
    
    # å¼‚å¸¸æ£€æµ‹
    if metrics['retrieval_latency'] > 1000:  # è¶…è¿‡1ç§’
        optimize_vector_index()
        
    if metrics['storage_utilization'] > 0.8:  # è¶…è¿‡80%
        trigger_memory_cleanup()
    
    return metrics
```

è¿™ä¸ªè®°å¿†ç³»ç»Ÿè®¾è®¡ä½¿INVESTOR-BENCHèƒ½å¤Ÿåƒäººç±»ä¸“å®¶ä¸€æ ·ç§¯ç´¯å’Œè¿ç”¨æŠ•èµ„çŸ¥è¯†ï¼Œå®ç°çœŸæ­£çš„æ™ºèƒ½åŒ–æŠ•èµ„å†³ç­–ã€‚