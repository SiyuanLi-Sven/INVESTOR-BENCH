#!/usr/bin/env python3
"""
æµ‹è¯•ç»Ÿä¸€é…ç½®ç³»ç»Ÿæ˜¯å¦æ­£å¸¸å·¥ä½œ
"""

import sys
import os

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(__file__))

def test_config_import():
    """æµ‹è¯•é…ç½®æ–‡ä»¶å¯¼å…¥"""
    try:
        from config import MODEL_CONFIGS, get_model_config, get_all_llm_models, get_all_embedding_models
        print("âœ“ é…ç½®æ–‡ä»¶å¯¼å…¥æˆåŠŸ")
        return True
    except Exception as e:
        print(f"âœ— é…ç½®æ–‡ä»¶å¯¼å…¥å¤±è´¥: {e}")
        return False

def test_model_configs():
    """æµ‹è¯•æ¨¡å‹é…ç½®"""
    try:
        from config import MODEL_CONFIGS, get_all_llm_models, get_all_embedding_models
        
        # æµ‹è¯•è·å–LLMæ¨¡å‹åˆ—è¡¨
        llm_models = get_all_llm_models()
        print(f"âœ“ æ‰¾åˆ° {len(llm_models)} ä¸ªLLMæ¨¡å‹: {llm_models}")
        
        # æµ‹è¯•è·å–Embeddingæ¨¡å‹åˆ—è¡¨  
        emb_models = get_all_embedding_models()
        print(f"âœ“ æ‰¾åˆ° {len(emb_models)} ä¸ªEmbeddingæ¨¡å‹: {emb_models}")
        
        # æµ‹è¯•è·å–ç‰¹å®šæ¨¡å‹é…ç½®
        if llm_models:
            from config import get_model_config
            test_model = llm_models[0]
            config = get_model_config(test_model)
            print(f"âœ“ æˆåŠŸè·å–æ¨¡å‹é…ç½®: {test_model}")
            print(f"  - API Base: {config['api_base']}")
            print(f"  - Provider: {config.get('provider', 'unknown')}")
        
        return True
    except Exception as e:
        print(f"âœ— æ¨¡å‹é…ç½®æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_embedding_unified():
    """æµ‹è¯•ç»Ÿä¸€embeddingæ¨¡å—"""
    try:
        from src.embedding_unified import UnifiedOpenAIEmbedding, EmbeddingModel
        print("âœ“ ç»Ÿä¸€Embeddingæ¨¡å—å¯¼å…¥æˆåŠŸ")
        return True
    except Exception as e:
        print(f"âœ— ç»Ÿä¸€Embeddingæ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
        return False

def test_openai_compatible():
    """æµ‹è¯•OpenAIå…¼å®¹æ¨¡å—"""
    try:
        from src.chat.endpoint.openai_compatible import OpenAICompatibleClient
        print("âœ“ OpenAIå…¼å®¹æ¨¡å—å¯¼å…¥æˆåŠŸ") 
        return True
    except Exception as e:
        print(f"âœ— OpenAIå…¼å®¹æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
        return False

def test_chat_module():
    """æµ‹è¯•chatæ¨¡å—æ›´æ–°"""
    try:
        from src.chat import get_chat_model
        from src.utils import TaskType
        print("âœ“ æ›´æ–°åçš„chatæ¨¡å—å¯¼å…¥æˆåŠŸ")
        return True
    except Exception as e:
        print(f"âœ— chatæ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
        return False

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("=" * 50)
    print("ç»Ÿä¸€é…ç½®ç³»ç»Ÿæµ‹è¯•")
    print("=" * 50)
    
    tests = [
        ("é…ç½®æ–‡ä»¶å¯¼å…¥", test_config_import),
        ("æ¨¡å‹é…ç½®", test_model_configs), 
        ("ç»Ÿä¸€Embeddingæ¨¡å—", test_embedding_unified),
        ("OpenAIå…¼å®¹æ¨¡å—", test_openai_compatible),
        ("Chatæ¨¡å—æ›´æ–°", test_chat_module),
    ]
    
    passed = 0
    total = len(tests)
    
    for test_name, test_func in tests:
        print(f"\n[{test_name}]")
        if test_func():
            passed += 1
        print("-" * 30)
    
    print(f"\næµ‹è¯•ç»“æœ: {passed}/{total} é€šè¿‡")
    
    if passed == total:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼ç»Ÿä¸€é…ç½®ç³»ç»Ÿå·²ç»å°±ç»ªã€‚")
        print("\nä½¿ç”¨è¯´æ˜:")
        print("1. åœ¨config.pyä¸­é…ç½®æ‚¨çš„æ¨¡å‹APIå¯†é’¥")
        print("2. åœ¨é…ç½®æ–‡ä»¶ä¸­è®¾ç½® 'chat_model_inference_engine': 'openai_compatible'")
        print("3. ä½¿ç”¨ 'chat_model': '<model_name>' æŒ‡å®šè¦ä½¿ç”¨çš„æ¨¡å‹")
        print("4. æ¨¡å‹åç§°å¿…é¡»åœ¨config.pyçš„MODEL_CONFIGSä¸­å®šä¹‰")
        return True
    else:
        print("âŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®ã€‚")
        return False

if __name__ == "__main__":
    main()